{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课堂笔记\n",
    "### 基本的RNN原理和实现\n",
    "全连接层的权重相对是最多的\n",
    "\n",
    "在卷积网络中卷积核共享，可以减少权重\n",
    "\n",
    "rnn  主要设计用来处理带有序列性质的数据  并且具有权重共享的概念（数据前后之间具有时间序列关系） ex：自然语言\n",
    "\n",
    "rnn cell 本质是一个线形层   实现向量维度的改变 这个线性层会一直反复的参与运算（权重在不同的序列之间共享），处理不同的序列信息。\n",
    "\n",
    "融合的概念：例如两者之间求和  做乘积 都可以叫做融合\n",
    "\n",
    "rnn中的激活函数：使用tanh比较多\n",
    "\n",
    "独热向量one-hot：缺点：1. 维度过高 2.向量稀疏 3. 硬编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNNcell单元 自己编写循环处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的依赖包\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行属性值的配置\n",
    "'''\n",
    "序列长度seq_len表示的是一共有多少个数据样本  \n",
    "输入维度input_size表示的一个样本里的向量维度是多少\n",
    "隐藏维度hidden_size表示的一个样本经过rnn计算以后新的维度\n",
    "'''\n",
    "batch_size  = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义rnn cell\n",
    "cell = torch.nn.RNNCell(input_size= input_size,hidden_size= hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成随机的数据\n",
    "dataset = torch.randn(seq_len, batch_size, input_size)\n",
    "# 指的是h0\n",
    "hidden = torch.zeros(batch_size,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n",
      "Input size: torch.Size([1, 4])\n",
      "output_size: torch.Size([1, 2])\n",
      "tensor([[-0.0512,  0.5675]], grad_fn=<TanhBackward>)\n",
      "==================== 1 ====================\n",
      "Input size: torch.Size([1, 4])\n",
      "output_size: torch.Size([1, 2])\n",
      "tensor([[-0.6816, -0.5774]], grad_fn=<TanhBackward>)\n",
      "==================== 2 ====================\n",
      "Input size: torch.Size([1, 4])\n",
      "output_size: torch.Size([1, 2])\n",
      "tensor([[0.8066, 0.4792]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 自己编写循环进行rnn计算\n",
    "# 根据代码可知使用的是tanh激活函数\n",
    "# note：这里使用的是一个cell因此out和hidden都是hidden\n",
    "\n",
    "for idx, input in enumerate(dataset):\n",
    "    print('='*20, idx,'='*20)\n",
    "    # 输入是一个1*4 的矩阵  batch_size, input_size 这表示的是一个序列\n",
    "    print('Input size:', input.shape) \n",
    "    # 进行计算 hidden是上一个序列输入的hidden  \n",
    "    hidden = cell(input,hidden)\n",
    "    # 输出维度是一个1*2的矩阵  batch_size,hidden_size \n",
    "    print('output_size:', hidden.shape)\n",
    "    print(hidden)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN直接进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用rnn进行计算\n",
    "# input 输入 整个序列 相比于使用cell的方式，不需要自己手写循环遍历序列\n",
    "# output 输出两个值 分别为out和hidden  out表示每一个序列对应的值  hidden即为综合之前所有序列在每一层的最终值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过代码观察输入、输出维度的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入依赖包\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数值\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机创建数据\n",
    "# input 即为输入的数据  \n",
    "# hidden 表示h0\n",
    "inputs = torch.randn(seq_len,batch_size,input_size)\n",
    "hidden = torch.zeros(num_layers,batch_size,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型 batch_first如果是true 输入中的第一个维度应该改为batch_size\n",
    "rnn = torch.nn.RNN(input_size=input_size, hidden_size = hidden_size,num_layers = num_layers,batch_first = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行计算\n",
    "out,hidden = rnn(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([3, 1, 2])\n",
      "output: tensor([[[ 0.1882,  0.2237]],\n",
      "\n",
      "        [[ 0.1690, -0.1969]],\n",
      "\n",
      "        [[ 0.5927, -0.5091]]], grad_fn=<StackBackward>)\n",
      "hidden size:  torch.Size([1, 1, 2])\n",
      "hidden: tensor([[[ 0.5927, -0.5091]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('output size: ',out.shape) # seq_len*batch_size*hidden_size\n",
    "# 关于输出维度的一点说明  seq_len 表示的是样本个数 所以在rnn过程中不会法神改变\n",
    "# hidden_size表示某个序列的向量维度 会随着rnn计算的进行发生变化\n",
    "print('output:',out)\n",
    "print('hidden size: ',hidden.shape)# num_layers*batch_size*hidden_size\n",
    "print('hidden:',hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 利用rnn进行seq2seq的序列转换\n",
    "hello->ohlol转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用rnncell自己编写循环进行转化的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖包\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置配置的初始值\n",
    "input_size = 4 # 只有四个不同的字符 因此使用one-hot编码方式四个维度即可\n",
    "hidden_size = 4\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建样本数据\n",
    "\n",
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "# x y 中的都表示在字典idx2char中的索引\n",
    "x_data = [1, 0, 2, 2, 3] # hello\n",
    "y_data = [3, 1, 2, 3, 2] # 0hlol\n",
    "\n",
    "# 进行one-hot 编码\n",
    "one_hot_lookup = [[1, 0, 0, 0],[0, 1, 0, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "# 调整数据维度 5*1*4 改为 seq_len*batch_size * input_size\n",
    "inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)\n",
    "# labels 输出值的标签seq_len*1 \n",
    "labels = torch.LongTensor(y_data).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型1 rnncell\n",
    "\n",
    "class Model_rnncell(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "      super(Model_rnncell,self).__init__()# 老师的课件中有误 修正一下\n",
    "      self.input_size = input_size\n",
    "      self.hidden_size = hidden_size\n",
    "      self.batch_size = batch_size\n",
    "      # 创建计算模型\n",
    "      self.rnncell = torch.nn.RNNCell(input_size = self.input_size,hidden_size = self.hidden_size)\n",
    "    \n",
    "    # 前向传播\n",
    "    def forward(self, input, hidden):\n",
    "        # 这里的参数hidden指的是上一个序列处理得到的hidden\n",
    "        hidden = self.rnncell(input,hidden)\n",
    "        return hidden\n",
    "    \n",
    "    # 实现初始化一个h0\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.batch_size,self.hidden_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "rnn_cell = Model_rnncell(input_size=input_size,hidden_size=hidden_size,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建损失\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(rnn_cell.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  l h e e l , Epoch [1/20] loss=7.4501\n",
      "Predicted string:  l h e h l , Epoch [2/20] loss=5.9269\n",
      "Predicted string:  l h l h l , Epoch [3/20] loss=4.8785\n",
      "Predicted string:  l h l h l , Epoch [4/20] loss=4.1979\n",
      "Predicted string:  o h l o l , Epoch [5/20] loss=3.7218\n",
      "Predicted string:  o h l o l , Epoch [6/20] loss=3.3757\n",
      "Predicted string:  o h l o l , Epoch [7/20] loss=3.0821\n",
      "Predicted string:  o h l o l , Epoch [8/20] loss=2.8174\n",
      "Predicted string:  o h l o l , Epoch [9/20] loss=2.5972\n",
      "Predicted string:  o h l o l , Epoch [10/20] loss=2.4399\n",
      "Predicted string:  o h l o l , Epoch [11/20] loss=2.3268\n",
      "Predicted string:  o h l o l , Epoch [12/20] loss=2.2222\n",
      "Predicted string:  o h l o l , Epoch [13/20] loss=2.1206\n",
      "Predicted string:  o h l o l , Epoch [14/20] loss=2.0418\n",
      "Predicted string:  o h l o l , Epoch [15/20] loss=1.9956\n",
      "Predicted string:  o h l o l , Epoch [16/20] loss=1.9650\n",
      "Predicted string:  o h l o l , Epoch [17/20] loss=1.9371\n",
      "Predicted string:  o h l o l , Epoch [18/20] loss=1.9129\n",
      "Predicted string:  o h l o l , Epoch [19/20] loss=1.8965\n",
      "Predicted string:  o h l o l , Epoch [20/20] loss=1.8852\n"
     ]
    }
   ],
   "source": [
    "# 开始模型训练 通过rnn实现两个维度的映射\n",
    "epochs = 20\n",
    "for epoch in range(epochs):# 15个epoch\n",
    "    # 初始化loss\n",
    "    loss = 0\n",
    "    # 优化器梯度归零\n",
    "    optimizer.zero_grad()\n",
    "    # 生成初始化隐藏维度 参数在初始化模型的时候已经传入\n",
    "    hidden = rnn_cell.init_hidden()\n",
    "    print('Predicted string: ', end = ' ')\n",
    "    # 每次循环取inputs中的一个序列 循环结束 序列取完\n",
    "    for input,label in zip(inputs,labels):\n",
    "        # 使用模型进行计算\n",
    "        hidden = rnn_cell(input,hidden)\n",
    "        # 每一步都可以看作构建计算图的一部分\n",
    "        loss += criterion(hidden, label)\n",
    "        # 找到最大的数所在的索引 idx是一个tensor类型数据\n",
    "        _,idx = hidden.max(dim =1)\n",
    "        print(idx2char[idx.item()],end = ' ')\n",
    "    # 反向传播计算梯度信息\n",
    "    loss.backward()\n",
    "    # 优化器进行优化\n",
    "    optimizer.step()\n",
    "    print(', Epoch [%d/%d] loss=%.4f' % (epoch+1, epochs,loss.item()))   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用rnn创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖包\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化配置参数\n",
    "input_size = 4 # 只有四个不同的字符 因此使用one-hot编码方式四个维度即可\n",
    "hidden_size = 4\n",
    "batch_size = 1\n",
    "seq_len = 5 # 直接使用rnn模块 所以需要给出每一层需要处理的序列长度（样本个数 这里是五个字母 所以设置为5）\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化样本数据\n",
    "\n",
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "# x y 中的都表示在字典idx2char中的索引\n",
    "x_data = [1, 0, 2, 2, 3] # hello\n",
    "y_data = [3, 1, 2, 3, 2] # 0hlol\n",
    "\n",
    "# 进行one-hot 编码\n",
    "one_hot_lookup = [[1, 0, 0, 0],[0, 1, 0, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "# 调整数据维度 5*1*4 改为 seq_len*batch_size * input_size\n",
    "inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)\n",
    "# labels 输出值的标签seq_len\n",
    "labels = torch.LongTensor(y_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建基于rnn的模型\n",
    "\n",
    "class RnnModel(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,batch_size,num_layers =1 ):# rnn默认一层\n",
    "        super(RnnModel,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        # 创建rnn模型 需要给出的三个参数\n",
    "        self.rnn = torch.nn.RNN(input_size = input_size,hidden_size = hidden_size,num_layers = num_layers)\n",
    "\n",
    "    # 前向传播过程\n",
    "    def forward(self,input):\n",
    "        # 这里首先创建一个初始化的h0\n",
    "        hidden = torch.zeros(self.num_layers,self.batch_size, self.hidden_size)\n",
    "        # note：对结果取用的一点说明 out指的是每一个序列 \n",
    "        # 再上一个模型中 进行训练得出的隐藏序列是每一个序列最后输出的结果\n",
    "        out,_ = self.rnn(input,hidden)\n",
    "        return out.view(-1,self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型实例化\n",
    "rnnModel = RnnModel(input_size,hidden_size,batch_size,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建损失\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(rnnModel.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  hllll, Epoch [1/15] loss = 1.345\n",
      "Predicted:  lllll, Epoch [2/15] loss = 1.228\n",
      "Predicted:  lllll, Epoch [3/15] loss = 1.131\n",
      "Predicted:  ohlll, Epoch [4/15] loss = 1.047\n",
      "Predicted:  ohlll, Epoch [5/15] loss = 0.978\n",
      "Predicted:  ohlll, Epoch [6/15] loss = 0.927\n",
      "Predicted:  ohlol, Epoch [7/15] loss = 0.886\n",
      "Predicted:  ohlol, Epoch [8/15] loss = 0.851\n",
      "Predicted:  ohlol, Epoch [9/15] loss = 0.820\n",
      "Predicted:  ohlol, Epoch [10/15] loss = 0.790\n",
      "Predicted:  ohlol, Epoch [11/15] loss = 0.763\n",
      "Predicted:  ohlll, Epoch [12/15] loss = 0.738\n",
      "Predicted:  ohlll, Epoch [13/15] loss = 0.717\n",
      "Predicted:  ohlll, Epoch [14/15] loss = 0.697\n",
      "Predicted:  ohlll, Epoch [15/15] loss = 0.679\n"
     ]
    }
   ],
   "source": [
    "# 训练过程\n",
    "epochs = 15\n",
    "for epoch in range (epochs):\n",
    "    # 梯度归零\n",
    "    optimizer.zero_grad()\n",
    "    # 计算模型输出\n",
    "    outputs = rnnModel(inputs)\n",
    "    # 计算模型损失\n",
    "    loss = criterion(outputs,labels)\n",
    "    # 反向传播梯度\n",
    "    loss.backward()\n",
    "    # 使用优化器进行迭代\n",
    "    optimizer.step()\n",
    "\n",
    "    # 得到结果最大的索引\n",
    "    _, idx = outputs.max(dim = 1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')\n",
    "    print(', Epoch [%d/%d] loss = %.3f' % (epoch + 1, epochs,loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用词嵌入解决one-hot编码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入依赖包\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置基本的参数值\n",
    "\n",
    "num_class = 4 # 类别数量 \n",
    "input_size = 4 # 输入的一个序列的维度（表示一个字母的维度  我们这里使用one-hot是四维的）\n",
    "hidden_size = 8 # 隐藏层的维度\n",
    "embedding_size = 10 # 引入嵌入以后的维度数据\n",
    "num_layers = 2 # rnn的层数\n",
    "batch_size = 1 # 批次\n",
    "seq_len = 5 # 序列长度（五个样本数据 即 五个单词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建样本数据\n",
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "x_data = [[1, 0, 2, 2, 3]] # (batch, seq_len)\n",
    "y_data = [3, 1, 2, 3, 2] # (batch * seq_len)\n",
    "inputs = torch.LongTensor(x_data)\n",
    "labels = torch.LongTensor(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class RnnEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RnnEmbeddingModel,self).__init__()\n",
    "        # 编写嵌入\n",
    "        self.embedding = torch.nn.Embedding(input_size,embedding_size)\n",
    "        # 创建rnn模型\n",
    "        self.rnn_emb = torch.nn.RNN(input_size = embedding_size,hidden_size = hidden_size,num_layers= num_layers,batch_first = True)\n",
    "        # 使用线性层作为最终的预测\n",
    "        self.fc = torch.nn.Linear(hidden_size,num_class)\n",
    "\n",
    "    # 前向传播过程\n",
    "    def forward(self,input):\n",
    "        '''\n",
    "        hidden shape :  torch.Size([2, 1, 8])\n",
    "        input shape :  torch.Size([1, 5, 10])\n",
    "        out shape :  torch.Size([1, 5, 8])\n",
    "        out shape :  torch.Size([1, 5, 4])\n",
    "        '''\n",
    "        # 初始化一个h0  shape: num_layers * batch_size * hidden_size\n",
    "        hidden = torch.zeros(num_layers,batch_size,hidden_size)\n",
    "        # print('hidden shape : ',hidden.shape)\n",
    "        # print(hidden)\n",
    "\n",
    "        # 进行embedding操作 shape: batch_size * seq_len * embedding_size\n",
    "        input = self.embedding(input)\n",
    "        # print('input shape : ',input.shape)\n",
    "        # print(input)\n",
    "\n",
    "        # 使用rnn进行计算 shape: batch_size * seq_len * output_size\n",
    "        out,_ = self.rnn_emb(input,hidden)\n",
    "        # print('out shape : ',out.shape)\n",
    "        # print(out)\n",
    "\n",
    "        # 通过线性层 shape: batch_size * seq_len * num_class\n",
    "        out = self.fc(out)\n",
    "        # print(out)\n",
    "        # print('out shape : ',out.shape)\n",
    "        \n",
    "        # 调整维度\n",
    "        return out.view(-1,num_class)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "rnn_Embedding = RnnEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(rnn_Embedding.parameters(), lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  lolle, Epoch [1/20] loss = 1.401\n",
      "Predicted:  ohlol, Epoch [2/20] loss = 1.069\n",
      "Predicted:  ohlol, Epoch [3/20] loss = 0.834\n",
      "Predicted:  ohlol, Epoch [4/20] loss = 0.589\n",
      "Predicted:  ohlol, Epoch [5/20] loss = 0.392\n",
      "Predicted:  ohlol, Epoch [6/20] loss = 0.250\n",
      "Predicted:  ohlol, Epoch [7/20] loss = 0.158\n",
      "Predicted:  ohlol, Epoch [8/20] loss = 0.102\n",
      "Predicted:  ohlol, Epoch [9/20] loss = 0.067\n",
      "Predicted:  ohlol, Epoch [10/20] loss = 0.045\n",
      "Predicted:  ohlol, Epoch [11/20] loss = 0.031\n",
      "Predicted:  ohlol, Epoch [12/20] loss = 0.022\n",
      "Predicted:  ohlol, Epoch [13/20] loss = 0.016\n",
      "Predicted:  ohlol, Epoch [14/20] loss = 0.012\n",
      "Predicted:  ohlol, Epoch [15/20] loss = 0.009\n",
      "Predicted:  ohlol, Epoch [16/20] loss = 0.007\n",
      "Predicted:  ohlol, Epoch [17/20] loss = 0.005\n",
      "Predicted:  ohlol, Epoch [18/20] loss = 0.004\n",
      "Predicted:  ohlol, Epoch [19/20] loss = 0.004\n",
      "Predicted:  ohlol, Epoch [20/20] loss = 0.003\n"
     ]
    }
   ],
   "source": [
    "# 开始训练过程\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # 优化器归零\n",
    "    optimizer.zero_grad()\n",
    "    # 使用模型进行计算\n",
    "    outputs = rnn_Embedding(inputs)\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs,labels)\n",
    "    # 沿着构造的计算图进行反向传播\n",
    "    loss.backward()\n",
    "    # 进行优化\n",
    "    optimizer.step()\n",
    "\n",
    "    # 获取每一个序列所得的最大值\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    \n",
    "    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')\n",
    "    print(', Epoch [%d/%d] loss = %.3f' % (epoch + 1,epochs, loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bba41e528b9ae6ac3bca14918216a30fd62a7a5cc8cd4a9b2bafdbd58ed8824d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('dlpython37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
