# pytorch 学习
## 项目主要是实现了刘老师的课程代码 [传送门](https://www.bilibili.com/video/BV1Y7411d7Ys?spm_id_from=0.0.header_right.fav_list.click)


# 01 第一讲  简介
    深度学习基本范围
    反向传播：核心是计算图。先经过正向的计算（前馈过程）同时计算每一步的偏导（链式法则）
    Pytorch构造动态图


# 02 第二讲 线性模型

- 基本步骤：数据集 模型 训练 推理

    已知输出值 （数据有标签）：监督学习

    测试集模型算出来的结果和已知的结果进行比较

    过拟合：在训练集上误差很少（噪声也被学进去了），  希望得到泛化能力

    为了测验泛化能力，将训练集继续分为两份（训练集和开发集==验证集）
$$ \hat y = w*x+b$$

    在机器学习中，权重首先随机选择，然后通过评估进行迭代，通过计算损失，

    损失函数针对某一个样本，代价函数的一个例子是MSE（平均平方误差）



# 03 第三讲 梯度下降
在搜索最优结果时采用分治的思想，但是由于代价函数不光滑，因此不能使用分治法进行寻找最优参数

优化问题：寻找目标函数的最优解

梯度：目标函数对权重求导数，结果为正 上升，反之下降（迭代的过程：贪心）不一定得到最优的结果，但是能得到局部最优的结果

两个问题1.局部最优   2. 鞍点（使用随机梯度下降可以解决 针对单个样本进行梯度下降）

画损失图像可以进行平滑

解决鞍点问题的方法：
随机梯度下降

随机梯度下降无法使用并行计算，因此时间消耗大 但是性能好（样本必须一个一个来！）

整体梯度下降可以进行并行计算 但是性能不好（并行算完  求平均即可）

为此进行折中  使用batch （Mini-Batch）若干个为一组 

# 04 第四讲 反向传播

训练时候更新权重：计算最终的损失对w的求导，进而对权重进行更新（使用随机梯度下降）

通过计算图的计算，在图上传播梯度，最终根据链式法则求出梯度，即反向传播算法

全连接网络中的一层：输入乘以权重，再加上偏置，得到输出结果

全连接网络如果不加激活函数，会可以化简成为线性层，因此要使用激活函数（非线性函数）

求导：链式求导法则

链式求导过程

$$ z = w*x +b$$ 

1. 创建计算图
2. 前馈运算，由输入向最终的loss进行计算，计算过程中对操作数（输入和权重）同时求导（整体上看就是求整体loss）
3. 反向传播：输出结果拿到最终损失对输出的偏导（上一步对z的偏导），使用链式法则进行计算
4. 总结：向前传播时计算各自局部的梯度（此时还是变量表示的）和通向下一步的输出值，最后的向前直接传播一步一步的梯度，因为反向传播的目标是计算最终的loss对各自的梯度，所以必须前向传播结束以后才能进行反向传播

在pytorch中：
数据类：Tensor，其中的两个成员  data和grad（损失对本数据的导数）。所有的数据都存放在Tensor中

Tensor中默认是不需要计算梯度的，所以需要手工设定

基于pytorch的代码可以理解为是在构建计算图

计算图在进行backward结束后会进行释放

反向传播结束后权重的梯度需要清零，否则下一次会进行求和。 

    在模型训练过程中，会多次计算loss，也就是对于你代码里的W（权重），我们每一次计算都会计算loss都会同时计算一次梯度，如果在一次反向传播结束以后，梯度没有置零，那么下一次计算loss的同时计算出的梯度会上一次的梯度进行累计，这不是对的，所以每一次计算结束后都需要梯度归零，（w.grad.data.zero()）

# 05 第五讲 使用pytorch线性回归

sgd：随机梯度下降 核心：计算每一个权重对每一个损失的梯度
本讲目标 使用pytorch工具进行线性回归

正向目标：算出损失；反向目标：算出梯度

step： 1 构建数据集 2 设计模型 3 构造损失函数和优化器 4 训练周期  前馈  反馈  更新  

广播机制： 两个矩阵不能直接做加法，将另一个矩阵扩充

构造计算图 确定W矩阵的大小和偏置的大小（根据IO的大小确定对应的维度）

损失loss必须是一个标量（反向传播也不必须是标量）

模型必须定义为一个类

必须继承自Module 实现两个函数   init和forword   module可以自动计算反向传播

Python 参数：* args；**kwargs

实例化自定义的模型即可

损失可以直接调用

优化器 实例化

# 06 第六讲 使用pytorch逻辑分类

分类问题

$$\frac{1}{1 + e^{-x}}$$

根据输入判断属于某一个类的概率是多少。目标：计算每一个类的概率值，寻找最大值

二分类问题的输出计算一个概率即可

想要的是一个概率 所以需要将实数空间内的输出映射到[0,1]之间，使用函数

$$\frac{1}{1 + e^{-x}}$$
sigmod函数其实指的是类函数，只要满足饱和函数条件的即可。

知识约定俗成  在pytorch中sigmod函数就是logistic函数 ，即上述公式。记作$$\sigma$$无参数

损失函数也应该相应的改变

$$loss = -(ylog\hat y + (1+y) log(1 - \hat y))$$ 
(加了负号  越小越好  BCE损失 binary cross entropy)

KL散度

cross-entropy交叉熵 表示两个分布之间差异性的大小

# 07 第七讲 多维输入

如果输入是多维的，只需要保证和矩阵相乘以后结果为标量即可
sigma函数是一个可以处理向量的激活函数

一个线性层可以看作是一个矩阵运算

一般而言中间隐层越多，学习能力越强（不是越强越好，否则会将噪声学到导致过拟合）

# 08 第八讲 数据集加载

dataloader

前摇概念

Epoch：所有的训练样本进行一次反向传播和正向传播

batch-size：每次训练用到的样本大小

iteration：epoch/batch-size

1. Shuffle 对数据进行打乱
2. loader对数据进行分组，每组为一个batch_size 可迭代的

两个表示数据集的类  dataset dataloader

dataset抽象类 不能实例化，到那但是可以自定义对dataser的子类，dataloader可以实例化

# 09 第九讲 多分类问题

 多分类问题

 最后的输出改为十维，分别计算属于每一个分类的概率（但是互相会抑制其他的）

Softmax 归一化处理 满足每一个类的预测都大于零，满足所有类的结果和为1

损失之一：交叉熵损失(其中包含一个softmax) 最后一层不需要进行激活（非线性变换）

# 10 第十讲 基础卷积神经网络

卷积层：保留数据空间特征（全连接丧失了原有的数据的空间信息）
下采样：通道数不变，宽度和高度改变；目的：减少元素数量，降低运算需求

卷积层和下采样都属于特征提取器的一部分

最后的全连接网络  叫做分类器

一般图像都是由栅格化组成，由光敏器件经过光照的捕捉，生成像素值 拼成图像

两种图像：1. 栅格图像2. 矢量图像（基本都是人工或者程序生成）

一般的彩色图像一定有三个通道 红绿蓝 以及宽度和高度

卷积过程中取图像块（patch）进行卷积 patch进行滑动遍历整个图像数据，其中出书的数据中每一个数据都包含了原来patch中的所有数据特征，一个卷积核kernel对应一个通道，输入中的patch上每一个数据都和kernel中的对应数据相乘

输入的通道数应该和卷积核的通道数一致，两者数乘。然后相同位置进行加和，得到一个通道

输出有想有几个通道，就设计几个卷积核

一般卷积核的长和宽的数量使用奇数

卷积层对输入的长和宽没有要求，但对通道数量有要求

Padding 进行填充

View 变换维度

Stride 步长

下采样：最大池化、平均池化等

最大池化没有权重 

在全连接层的最后进行view展平卷积  以便接入全连接分类器

使用交叉熵损失，最后一次不进行激活

# 11 第十一讲 高级卷积神经网络

减少代码冗余：使用函数/类

Google net   不同的路径必须保证长和宽一致  只有通道数量可以不一样

信息融合：不同的通道相乘后相加 就可以看作一种信息的融合

1*1的卷积 降低运算量

Residual 网络 解决梯度消失问题：实现 经过权重层以后和x做加法 ；残差块网络

最后先求和  再激活

学习步骤：

1. 阅读理论数据  《深度学习花树》
2. 阅读pytorch文档 通读一遍
3. 复现经典工作 读代码
4. 扩充视野 

# 12 第十二讲 基础循环神经网络

全连接 Dense Deep 权重最多

专门处理带有序列的数据，运用权重共享的概念 。

Rnn 本质上也是一个线性层 实现维度的变换

在循环神经网络中的激活函数基本使用tanh

run运算比较耗时

    独热向量one-hot：缺点：1. 维度过高 2.向量稀疏 3. 硬编码

目的：找到一种稠密，低维空间之中  找到了 embedding （数据降维）

加入嵌入层 embedding要求输出和输出两个的维度

LSTM GRU
1. 序列的data
2. 循环过程中的权重共享

# 13 第十三讲 高级循环神经网络

基于rnn的分类器 

一般过程
1. 文本嵌入
2. GRU层
3. 得到最后一个输出
4. 线形层进行分类预测

双向的循环网络。考虑后面对前面的数据的影响

如果是双向的   需要在计算结束后进行拼接

原始数据转化为适合计算的维度，进行维度的变换


